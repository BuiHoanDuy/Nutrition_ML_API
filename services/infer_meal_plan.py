"""
Inference service for meal plan recommendations.

This script loads the pre-trained models and processed data from
`meal_plan_recommender_train.py` to provide meal plan recommendations
based on user's health status, goals, and nutrition intent.
""" 
import os
import pandas as pd
import joblib
from pathlib import Path
import scipy.sparse
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import httpx # For making asynchronous HTTP requests to OpenRouter
import json # For handling JSON data
import logging # For logging LLM interactions
from logging.handlers import RotatingFileHandler # For rotating log files

# Define paths
HERE = Path(__file__).parent.parent
MODEL_DIR = HERE / "services" / "models" / "meal_recommender"

# --- Setup Logging for LLM interactions ---
LOGS_DIR = Path(__file__).parent.parent / "logs" # Adjust path as needed, assuming logs dir is at project root
LOGS_DIR.mkdir(exist_ok=True)

llm_logger = logging.getLogger("llm_interactions")
llm_logger.setLevel(logging.INFO)
llm_logger.propagate = False # Prevent logs from propagating to the root logger
llm_handler = RotatingFileHandler(
    LOGS_DIR / "llm_interactions.log", maxBytes=10_000_000, backupCount=5, encoding="utf-8"
)
llm_logger.addHandler(llm_handler)

# Global variables to store loaded artifacts
meal_plans_df = None
user_feature_encoder = None
meal_tfidf_vectorizer = None
user_features_encoded_matrix = None
meal_features_tfidf_matrix = None
user_features_cols = ['t√¨nh_tr·∫°ng_s·ª©c_kh·ªèe', 'm·ª•c_ti√™u', 'intent_dinh_d∆∞·ª°ng']

def load_recommender_artifacts():
    """Loads all necessary artifacts for the meal plan recommender."""
    global meal_plans_df, user_feature_encoder, meal_tfidf_vectorizer, \
           user_features_encoded_matrix, meal_features_tfidf_matrix

    if meal_plans_df is not None: # Already loaded
        return

    print("üîπ Loading meal plan recommender artifacts...")
    try:
        meal_plans_df = pd.read_csv(MODEL_DIR / "meal_plans_data.csv", encoding='utf-8-sig')
        user_feature_encoder = joblib.load(MODEL_DIR / "user_feature_encoder.pkl")
        meal_tfidf_vectorizer = joblib.load(MODEL_DIR / "meal_tfidf_vectorizer.pkl")
        user_features_encoded_matrix = scipy.sparse.load_npz(MODEL_DIR / "user_features_encoded.npz")
        meal_features_tfidf_matrix = scipy.sparse.load_npz(MODEL_DIR / "meal_features_tfidf.npz")
        
        print("‚úÖ Meal plan recommender artifacts loaded successfully.")

    except FileNotFoundError as e:
        print(f"‚ùå Error loading meal plan recommender artifacts: {e}")
        print("Please ensure 'meal_plan_recommender_train.py' has been run successfully.")
        # Reset globals to None to indicate failure
        meal_plans_df = None
        user_feature_encoder = None
        meal_tfidf_vectorizer = None
        user_features_encoded_matrix = None
        meal_features_tfidf_matrix = None
    except Exception as e:
        print(f"‚ùå An unexpected error occurred while loading artifacts: {e}")
        meal_plans_df = None
        user_feature_encoder = None
        meal_tfidf_vectorizer = None
        user_features_encoded_matrix = None
        meal_features_tfidf_matrix = None


# --- OpenRouter Integration ---
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_MODEL = os.getenv("OPENROUTER_MODEL", "mistralai/mistral-7b-instruct-v0.2") # Default model
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

async def call_openrouter_llm(prompt: str, model: str = OPENROUTER_MODEL, json_mode: bool = True) -> dict | str | None:
    """
    Calls the OpenRouter LLM API to get a structured response.
    """
    if not OPENROUTER_API_KEY:
        llm_logger.warning("OPENROUTER_API_KEY not set. Skipping LLM call.")
        return None

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1 if json_mode else 0.7, # Low temp for JSON, higher for creative text
    }
    if json_mode:
        data["response_format"] = {"type": "json_object"} # Request JSON output

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(OPENROUTER_URL, headers=headers, json=data, timeout=30.0)
            response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)
            response_json = response.json()
            
            llm_response_content = response_json["choices"][0]["message"]["content"]
            llm_logger.info(f"LLM call successful. Model: {model}, Prompt: {prompt[:100]}..., Raw Response: {llm_response_content}")

            if json_mode:
                # Attempt to parse the LLM's string response as JSON
                parsed_llm_output = json.loads(llm_response_content)
                return parsed_llm_output
            else:
                # Return the natural language string directly
                return llm_response_content.strip()
    except httpx.RequestError as e:
        llm_logger.error(f"HTTPX Request Error calling OpenRouter LLM: {e}")
        return None
    except httpx.HTTPStatusError as e:
        llm_logger.error(f"HTTP Status Error from OpenRouter LLM: {e.response.status_code} - {e.response.text}")
        return None
    except (KeyError, IndexError, json.JSONDecodeError) as e:
        llm_logger.error(f"Error parsing LLM response or malformed response: {e}. Response: {response.text if 'response' in locals() else 'N/A'}")
        return None
    except Exception as e:
        llm_logger.error(f"An unexpected error occurred during LLM call: {e}")
        return None


async def parse_meal_plan_question(question: str) -> dict:
    """
    Parses a natural language question to extract health_status, goal, and nutrition_intent.
    Prioritizes LLM parsing via OpenRouter, falls back to keyword matching if LLM fails.
    """
    # 1. Attempt LLM parsing
    llm_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω dinh d∆∞·ª°ng. H√£y ph√¢n t√≠ch c√¢u h·ªèi sau c·ªßa ng∆∞·ªùi d√πng v√† tr√≠ch xu·∫•t c√°c th√¥ng tin sau v√†o ƒë·ªãnh d·∫°ng JSON:
- `health_status`: T√¨nh tr·∫°ng s·ª©c kh·ªèe (v√≠ d·ª•: 'b√©o ph√¨', 'ti·ªÉu ƒë∆∞·ªùng', 'b√¨nh th∆∞·ªùng', 'kh√¥ng c√≥').
- `goal`: M·ª•c ti√™u (v√≠ d·ª•: 'gi·∫£m c√¢n', 'tƒÉng c√¢n', 'tƒÉng c∆°', 'gi·ªØ c√¢n', 'kh√¥ng c√≥').
- `nutrition_intent`: √ù ƒë·ªãnh dinh d∆∞·ª°ng (v√≠ d·ª•: 'ƒÉn √≠t tinh b·ªôt', 'ƒÉn nhi·ªÅu protein', 'ƒÉn chay', '·ªïn ƒë·ªãnh ƒë∆∞·ªùng huy·∫øt', 'kh√¥ng c√≥').
- `requested_meals`: Danh s√°ch c√°c b·ªØa ƒÉn ƒë∆∞·ª£c y√™u c·∫ßu (v√≠ d·ª•: ['b·ªØa_s√°ng', 'b·ªØa_tr∆∞a', 'b·ªØa_t·ªëi', 'b·ªØa_ph·ª•']). N·∫øu kh√¥ng r√µ, m·∫∑c ƒë·ªãnh l√† t·∫•t c·∫£.

N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ, h√£y s·ª≠ d·ª•ng gi√° tr·ªã 'kh√¥ng c√≥' cho c√°c tr∆∞·ªùng `health_status`, `goal`, `nutrition_intent`.
ƒê·ªëi v·ªõi `requested_meals`, n·∫øu kh√¥ng c√≥ y√™u c·∫ßu c·ª• th·ªÉ, h√£y m·∫∑c ƒë·ªãnh l√† `['b·ªØa_s√°ng', 'b·ªØa_tr∆∞a', 'b·ªØa_t·ªëi', 'b·ªØa_ph·ª•']`.

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: '{question}'

Vui l√≤ng ch·ªâ tr·∫£ v·ªÅ m·ªôt ƒë·ªëi t∆∞·ª£ng JSON h·ª£p l·ªá.
"""
    llm_parsed_params = await call_openrouter_llm(llm_prompt)

    if llm_parsed_params:
        llm_logger.info(f"LLM successfully parsed question: {question} -> {llm_parsed_params}")
        # Validate LLM output structure and fill defaults if necessary
        final_params = {
            "health_status": llm_parsed_params.get("health_status", "kh√¥ng c√≥"),
            "goal": llm_parsed_params.get("goal", "kh√¥ng c√≥"),
            "nutrition_intent": llm_parsed_params.get("nutrition_intent", "kh√¥ng c√≥"),
            "requested_meals": llm_parsed_params.get("requested_meals", ["b·ªØa_s√°ng", "b·ªØa_tr∆∞a", "b·ªØa_t·ªëi", "b·ªØa_ph·ª•"])
        }
        # Ensure requested_meals is a list
        if not isinstance(final_params["requested_meals"], list):
            llm_logger.warning(f"LLM returned non-list for requested_meals. Defaulting. Raw: {llm_parsed_params.get('requested_meals')}")
            final_params["requested_meals"] = ["b·ªØa_s√°ng", "b·ªØa_tr∆∞a", "b·ªØa_t·ªëi", "b·ªØa_ph·ª•"]
        return final_params
    else:
        llm_logger.warning(f"LLM parsing failed for question: {question}. Falling back to keyword parsing.")
        # 2. Fallback to keyword matching if LLM fails
        # (Existing keyword parsing logic starts here)
    question_lower = question.lower()

    extracted_params = {
        "health_status": "kh√¥ng c√≥", # Default value if not found
        "goal": "kh√¥ng c√≥",
        "nutrition_intent": "kh√¥ng c√≥",
        "requested_meals": ["b·ªØa_s√°ng", "b·ªØa_tr∆∞a", "b·ªØa_t·ªëi", "b·ªØa_ph·ª•"] # Default to all meals if not found
    }

    # Define keywords for each category. Longer phrases should be checked first.
    # Keywords for t√¨nh_tr·∫°ng_s·ª©c_kh·ªèe
    health_status_map = {
        "b√©o ph√¨": ["b√©o ph√¨", "th·ª´a c√¢n"],
        "cao huy·∫øt √°p": ["cao huy·∫øt √°p", "huy·∫øt √°p cao"],
        "ti·ªÉu ƒë∆∞·ªùng": ["ti·ªÉu ƒë∆∞·ªùng", "ƒë∆∞·ªùng huy·∫øt cao"],
        "g·∫ßy y·∫øu": ["g·∫ßy y·∫øu", "qu√° g·∫ßy"],
        "b√¨nh th∆∞·ªùng": ["b√¨nh th∆∞·ªùng", "ng∆∞·ªùi b√¨nh th∆∞·ªùng"]
    }

    # Keywords for m·ª•c_ti√™u (Goal)
    goal_map = {
        "gi·∫£m c√¢n": ["gi·∫£m c√¢n", "mu·ªën gi·∫£m c√¢n"],
        "tƒÉng c√¢n": ["tƒÉng c√¢n", "mu·ªën tƒÉng c√¢n"],
        "tƒÉng c∆°": ["tƒÉng c∆°", "x√¢y d·ª±ng c∆° b·∫Øp"],
        "gi·ªØ c√¢n": ["gi·ªØ c√¢n", "duy tr√¨ c√¢n n·∫∑ng"],
        "duy tr√¨ s·ª©c kh·ªèe": ["duy tr√¨ s·ª©c kh·ªèe", "s·ª©c kh·ªèe t·ªët"],
        "ƒÉn l√†nh m·∫°nh": ["ƒÉn l√†nh m·∫°nh", "ch·∫ø ƒë·ªô ƒÉn l√†nh m·∫°nh"]
    }

    # Keywords for intent_dinh_d∆∞·ª°ng (Nutrition Intent)
    nutrition_intent_map = {
        "ƒÉn √≠t tinh b·ªôt": ["√≠t tinh b·ªôt", "low carb"],
        "ƒÉn nhi·ªÅu protein": ["nhi·ªÅu protein", "high protein"],
        "·ªïn ƒë·ªãnh ƒë∆∞·ªùng huy·∫øt": ["·ªïn ƒë·ªãnh ƒë∆∞·ªùng huy·∫øt"],
        "ƒÉn chay": ["ƒÉn chay", "chay"],
        # These are also goals, but can be explicit intents.
        "gi·∫£m c√¢n": ["gi·∫£m c√¢n"],
        "tƒÉng c√¢n": ["tƒÉng c√¢n"],
        "tƒÉng c∆°": ["tƒÉng c∆°"],
        "gi·ªØ c√¢n": ["gi·ªØ c√¢n"],
        "duy tr√¨ s·ª©c kh·ªèe": ["duy tr√¨ s·ª©c kh·ªèe"],
        "ƒÉn l√†nh m·∫°nh": ["ƒÉn l√†nh m·∫°nh"]
    }

    # Keywords for requested meals
    meal_map = {
        "b·ªØa_s√°ng": ["b·ªØa s√°ng", "bu·ªïi s√°ng", "s√°ng"],
        "b·ªØa_tr∆∞a": ["b·ªØa tr∆∞a", "bu·ªïi tr∆∞a", "tr∆∞a"],
        "b·ªØa_t·ªëi": ["b·ªØa t·ªëi", "bu·ªïi t·ªëi", "t·ªëi"],
        "b·ªØa_ph·ª•": ["b·ªØa ph·ª•", "ƒÉn v·∫∑t", "ƒÉn nh·∫π"],
        "c·∫£_ng√†y": ["c·∫£ ng√†y", "m·ªôt ng√†y", "1 ng√†y"]
    }

    # Helper to find keyword
    def find_keyword(text, keyword_map):
        for category, kws in keyword_map.items():
            # Sort keywords by length descending to match longer phrases first
            kws_sorted = sorted(kws, key=len, reverse=True)
            for kw in kws_sorted:
                if kw in text:
                    return category
        return "kh√¥ng c√≥"

    # Helper to find requested meals
    def find_requested_meals(text, meal_keyword_map):
        found_meals = []
        for meal_key, kws in meal_keyword_map.items():
            for kw in kws:
                if kw in text:
                    if meal_key == "c·∫£_ng√†y":
                        return ["b·ªØa_s√°ng", "b·ªØa_tr∆∞a", "b·ªØa_t·ªëi", "b·ªØa_ph·ª•"] # Return all if "c·∫£ ng√†y" is found
                    if meal_key not in found_meals:
                        found_meals.append(meal_key)
        return found_meals if found_meals else None

    extracted_params["health_status"] = find_keyword(question_lower, health_status_map)
    extracted_params["goal"] = find_keyword(question_lower, goal_map)
    extracted_params["nutrition_intent"] = find_keyword(question_lower, nutrition_intent_map)
    requested = find_requested_meals(question_lower, meal_map)

    # Fallback for nutrition_intent if not explicitly found
    if extracted_params["nutrition_intent"] == "kh√¥ng c√≥":
        # If the extracted goal is also a valid nutrition intent, use it.
        # This handles cases like "gi·∫£m c√¢n" being both a goal and an intent.
        if extracted_params["goal"] in ["gi·∫£m c√¢n", "tƒÉng c√¢n", "tƒÉng c∆°", "gi·ªØ c√¢n", "duy tr√¨ s·ª©c kh·ªèe", "ƒÉn l√†nh m·∫°nh"]:
            extracted_params["nutrition_intent"] = extracted_params["goal"]
        # Add more specific defaults if needed, e.g., if goal is "gi·∫£m c√¢n" and no intent, default to "ƒÉn √≠t tinh b·ªôt"
        # For now, using the goal as intent is a good starting point.

    if requested:
        extracted_params["requested_meals"] = requested

    # If any parameter is still "kh√¥ng c√≥" and the dataset has a default for it,
    # the recommender's similarity search will handle it by finding the closest match.

    return extracted_params



def recommend_meal_plan(
    health_status: str,
    goal: str,
    nutrition_intent: str,
    requested_meals: list[str] = None
) -> list[dict]:
    """
    Recommends the single best meal plan based on user's health status, goal, 
    and nutrition intent by finding the highest cosine similarity score.

    Args:
        health_status (str): T√¨nh tr·∫°ng s·ª©c kh·ªèe c·ªßa ng∆∞·ªùi d√πng.
        goal (str): M·ª•c ti√™u c·ªßa ng∆∞·ªùi d√πng (v√≠ d·ª•: gi·∫£m c√¢n, tƒÉng c√¢n).
        nutrition_intent (str): Intent dinh d∆∞·ª°ng c·ªßa ng∆∞·ªùi d√πng.
        requested_meals (list[str], optional): Danh s√°ch c√°c b·ªØa ƒÉn mu·ªën g·ª£i √Ω. M·∫∑c ƒë·ªãnh l√† t·∫•t c·∫£.

    Returns:
        list[dict]: Danh s√°ch c√°c th·ª±c ƒë∆°n ƒë∆∞·ª£c g·ª£i √Ω.
    """
    load_recommender_artifacts() # Ensure artifacts are loaded

    if meal_plans_df is None or user_feature_encoder is None:
        return [] # Return empty if artifacts failed to load

    if not requested_meals:
        requested_meals = ['b·ªØa_s√°ng', 'b·ªØa_tr∆∞a', 'b·ªØa_t·ªëi', 'b·ªØa_ph·ª•']

    # --- Unified Logic: Always use cosine similarity to find the single best match ---
    
    # 1. Create a DataFrame for the user's input to encode
    user_input_df = pd.DataFrame([{
        't√¨nh_tr·∫°ng_s·ª©c_kh·ªèe': health_status,
        'm·ª•c_ti√™u': goal,
        'intent_dinh_d∆∞·ª°ng': nutrition_intent
    }])
    
    # 2. Encode the user's input
    try:
        user_input_encoded = user_feature_encoder.transform(user_input_df[user_features_cols])
    except ValueError as e:
        print(f"‚ùå Error encoding user input: {e}")
        return []

    # 3. Calculate similarity between user input and all existing user profiles
    user_similarity_scores = cosine_similarity(user_input_encoded, user_features_encoded_matrix).flatten()
    
    # 4. Get the index of the single best match (highest similarity score)
    best_match_index = user_similarity_scores.argmax()
    
    # 5. Retrieve the corresponding meal plan
    best_plan_raw = meal_plans_df.iloc[best_match_index:best_match_index+1].to_dict(orient='records')

    # Format the output to include only relevant meal plan details
    output_recommendations = []
    for plan in best_plan_raw:
        recommendation = {
            "t√¨nh_tr·∫°ng_s·ª©c_kh·ªèe": plan.get('t√¨nh_tr·∫°ng_s·ª©c_kh·ªèe', 'N/A'),
            "m·ª•c_ti√™u": plan.get('m·ª•c_ti√™u', 'N/A'),
            "intent_dinh_d∆∞·ª°ng": plan.get('intent_dinh_d∆∞·ª°ng', 'N/A'),
            "t·ªïng_calo": plan.get('t·ªïng_calo', 'N/A'),
            "ghi_ch√∫": plan.get('ghi_ch√∫', 'N/A')
        }
        for meal_key in requested_meals:
            recommendation[meal_key] = plan.get(meal_key) or "Kh√¥ng c√≥ g·ª£i √Ω"
        output_recommendations.append(recommendation)
    
    return output_recommendations

async def generate_natural_response_from_recommendations(question: str, recommendations: list[dict]) -> str:
    """
    Uses an LLM via OpenRouter to generate a natural, conversational response 
    based on the structured meal recommendations.

    Args:
        question (str): The original user question.
        recommendations (list[dict]): The list of recommendation dictionaries from `recommend_meal_plan`.

    Returns:
        str: A natural language response.
    """
    if not recommendations:
        return "R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y th·ª±c ƒë∆°n n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i v·ªõi m·ªôt y√™u c·∫ßu kh√°c nh√©."

    # Convert recommendations to a string format for the prompt
    recs_str = ""
    # Since we now only have one best recommendation, we can simplify the prompt
    if recommendations:
        rec = recommendations[0]
        # Only include meals that were requested and have a valid suggestion
        for meal_key in ['b·ªØa_s√°ng', 'b·ªØa_tr∆∞a', 'b·ªØa_t·ªëi', 'b·ªØa_ph·ª•']:
            if meal_key in rec and rec[meal_key] and rec[meal_key] != "Kh√¥ng c√≥ g·ª£i √Ω":
                recs_str += f"- {meal_key.replace('_', ' ').capitalize()}: {rec[meal_key]}\n"
        recs_str += f"- T·ªïng calo (∆∞·ªõc t√≠nh): {rec.get('t·ªïng_calo', 'N/A')} kcal\n"
        recs_str += f"- Ghi ch√∫: {rec.get('ghi_ch√∫', 'Kh√¥ng c√≥')}\n"

    prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia dinh d∆∞·ª°ng AI th√¢n thi·ªán v√† am hi·ªÉu.
Ng∆∞·ªùi d√πng v·ª´a h·ªèi: "{question}"

D·ª±a tr√™n d·ªØ li·ªáu, ƒë√¢y l√† g·ª£i √Ω th·ª±c ƒë∆°n ph√π h·ª£p nh·∫•t cho h·ªç:
{recs_str}
Nhi·ªám v·ª• c·ªßa b·∫°n l√† di·ªÖn gi·∫£i nh·ªØng g·ª£i √Ω tr√™n th√†nh m·ªôt c√¢u tr·∫£ l·ªùi t·ª± nhi√™n, m∆∞·ª£t m√†, v√† mang t√≠nh t∆∞ v·∫•n cho ng∆∞·ªùi d√πng.

**Y√™u c·∫ßu:**
1.  **B·∫Øt ƒë·∫ßu th√¢n thi·ªán**: Ch√†o h·ªèi v√† t√≥m t·∫Øt l·∫°i y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng (v√≠ d·ª•: "Ch√†o b·∫°n, ƒë·ªÉ gi√∫p b·∫°n tƒÉng c∆° hi·ªáu qu·∫£...").
2.  **Tr√¨nh b√†y r√µ r√†ng**: Li·ªát k√™ c√°c m√≥n ƒÉn g·ª£i √Ω cho t·ª´ng b·ªØa m·ªôt c√°ch m·∫°ch l·∫°c.
3.  **Gi·∫£i th√≠ch ng·∫Øn g·ªçn**: N√™u l√Ω do t·∫°i sao th·ª±c ƒë∆°n n√†y ph√π h·ª£p (v√≠ d·ª•: "Th·ª±c ƒë∆°n n√†y t·∫≠p trung v√†o c√°c m√≥n gi√†u protein nh∆∞ c√° v√† ·ª©c g√†, r·∫•t t·ªët cho vi·ªác x√¢y d·ª±ng c∆° b·∫Øp...").
4.  **Th√™m l·ªùi khuy√™n**: D·ª±a v√†o ph·∫ßn "Ghi ch√∫" ƒë·ªÉ ƒë∆∞a ra l·ªùi khuy√™n b·ªï sung.
5.  **K·∫øt th√∫c ƒë·ªông vi√™n**: K·∫øt th√∫c b·∫±ng m·ªôt l·ªùi ch√∫c ho·∫∑c ƒë·ªông vi√™n t√≠ch c·ª±c.

**Quan tr·ªçng**: C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n ph·∫£i l√† m·ªôt ƒëo·∫°n vƒÉn ho√†n ch·ªânh, kh√¥ng s·ª≠ d·ª•ng c√°c k√Ω t·ª± g·∫°ch ƒë·∫ßu d√≤ng (`-`) hay xu·ªëng d√≤ng kh√¥ng c·∫ßn thi·∫øt (`\n`). H√£y vi·∫øt nh∆∞ m·ªôt chuy√™n gia ƒëang tr√≤ chuy·ªán tr·ª±c ti·∫øp v·ªõi ng∆∞·ªùi d√πng.
"""
    # Call the LLM in non-JSON mode to get a natural language response
    natural_response = await call_openrouter_llm(
        prompt,
        model=os.getenv("OPENROUTER_MODEL_CHAT", "openai/gpt-3.5-turbo"), # Use a potentially different model for chat
        json_mode=False
    )

    if natural_response and isinstance(natural_response, str):
        # Th√™m m·ªôt b∆∞·ªõc l√†m s·∫°ch cu·ªëi c√πng ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≤n k√Ω t·ª± kh√¥ng mong mu·ªën
        return natural_response.replace("\n", " ").replace("- ", "").strip()
    
    llm_logger.warning("LLM response for natural language generation was not in the expected format. Falling back to raw data.")
    # Fallback if LLM fails
    # T·∫°o c√¢u tr·∫£ l·ªùi d·ª± ph√≤ng v√† l√†m s·∫°ch n√≥ ngay l·∫≠p t·ª©c.
    fallback_response = f"D∆∞·ªõi ƒë√¢y l√† c√°c g·ª£i √Ω d√†nh cho b·∫°n: {recs_str}"
    # X·ª≠ l√Ω chu·ªói ƒë·ªÉ c√≥ ƒë·ªãnh d·∫°ng ƒë·∫πp h∆°n:
    # 1. Lo·∫°i b·ªè g·∫°ch ƒë·∫ßu d√≤ng "- ".
    # 2. Thay th·∫ø k√Ω t·ª± xu·ªëng d√≤ng b·∫±ng ", " ƒë·ªÉ ngƒÉn c√°ch c√°c √Ω.
    # 3. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng v√† d·∫•u ph·∫©y th·ª´a ·ªü ƒë·∫ßu/cu·ªëi chu·ªói.
    cleaned_fallback = fallback_response.replace('- ', '').replace('\n', ', ').strip().rstrip(',').strip()
    return cleaned_fallback

if __name__ == "__main__":
    import asyncio # Required for running async functions

    async def main_test_infer_meal_plan():
        # Example usage
        print("--- Test Recommendation ---")
        # Example 1: Should find exact matches
        recs = recommend_meal_plan(
            health_status="b√¨nh th∆∞·ªùng",
            goal="gi·∫£m c√¢n",
            nutrition_intent="ƒÉn √≠t tinh b·ªôt"
        )
        print("\nRecommendations for 'B√¨nh th∆∞·ªùng', 'Gi·∫£m c√¢n', 'ƒÇn √≠t tinh b·ªôt':")
        if recs:
            for r in recs:
                print(f"- S√°ng: {r.get('b·ªØa_s√°ng')}, Tr∆∞a: {r.get('b·ªØa_tr∆∞a')}, T·ªëi: {r.get('b·ªØa_t·ªëi')}, Calo: {r.get('t·ªïng_calo')}")
        else:
            print("No recommendations found.")

        print("\n--- Test Question Parsing ---")
        question_example = "T√¥i b·ªã b√©o ph√¨, s√°ng n√™n ƒÉn g√¨ ƒë·ªÉ gi·∫£m c√¢n?"
        # Await the async function call
        parsed_params = await parse_meal_plan_question(question_example)
        print(f"Parsed from '{question_example}': {parsed_params}")
        
        # Use parsed parameters to get recommendations
        recs_from_question = recommend_meal_plan(**parsed_params)
        print("\nRecommendations from parsed question:")
        if recs_from_question:
            for r in recs_from_question:
                print(f"- S√°ng: {r.get('b·ªØa_s√°ng')}, Tr∆∞a: {r.get('b·ªØa_tr∆∞a')}, T·ªëi: {r.get('b·ªØa_t·ªëi')}, Calo: {r.get('t·ªïng_calo')}")
        else:
            print("No recommendations found.")

        # Example 2: Test with a more complex question that LLM should handle better
        question_complex = "T√¥i l√† ng∆∞·ªùi b√¨nh th∆∞·ªùng, mu·ªën tƒÉng c∆° th√¨ b·ªØa tr∆∞a v√† b·ªØa t·ªëi n√™n ƒÉn g√¨, ∆∞u ti√™n nhi·ªÅu protein?"
        parsed_params_complex = await parse_meal_plan_question(question_complex)
        print(f"\nParsed from '{question_complex}': {parsed_params_complex}")
        recs_complex = recommend_meal_plan(**parsed_params_complex)
        print("\nRecommendations from complex question:")
        if recs_complex:
            for r in recs_complex:
                print(f"- S√°ng: {r.get('b·ªØa_s√°ng')}, Tr∆∞a: {r.get('b·ªØa_tr∆∞a')}, T·ªëi: {r.get('b·ªØa_t·ªëi')}, Calo: {r.get('t·ªïng_calo')}")
        else:
            print("No recommendations found.")

    asyncio.run(main_test_infer_meal_plan()) # Run the async test function
